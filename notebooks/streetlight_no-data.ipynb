{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9ab7ec02f5f94295811227848603a689",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3680,
    "execution_start": 1715143363976,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# ! pip install edsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0cfa61a554a2460390d7cc69ca8e57d3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 334,
    "execution_start": 1715143367659,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDSL has a cache directory that stores the responses from the LLM. Since we need new responses everytime, we need to clear the cache.    \n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to the .edsl_cache directory\n",
    "cache_dir = '.edsl_cache'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(cache_dir):\n",
    "    # Remove the directory and its contents\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "689b7cfec72646ccbcfb4a6639cd27b0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1008,
    "execution_start": 1715143367704,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from edsl.questions import QuestionMultipleChoice\n",
    "from edsl import Agent, Model, Cache, Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the model you want to use here\n",
    "model = Model('gpt-4-turbo')\n",
    "#Set the rate limits for the model, lower tier OpenAI accounts have lower rate limits, therfore we need to explicitly set the rate limits for the model\n",
    "model._set_rate_limits(rpm=10000, tpm=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c98e2949ba75483ba2bc20f244298f64",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 86,
    "execution_start": 1715143368715,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "MOUNTAINS_DISTRIBUTION = [\"low\", \"low\", \"low\", \"medium\", \"high\"]  # Distribution of mountains\n",
    "GEMS_MAPPING = {\"low\":\"Topaz\", \"medium\":\"Ruby\", \"high\":\"Diamond\"} #Mapping of the mountains to the gems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_players = 5 #Number of players in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent traits, note that we can create a large number of agents and use only a subset of them in the experiment. \n",
    "agent_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "ages = [22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "locations = [\"New York\", \"California\", \"Texas\", \"Florida\", \"Washington\", \"Nevada\", \"Oregon\", \"Colorado\", \"Arizona\", \"Illinois\"]\n",
    "names = [\"Jack\", \"Jane\", \"David\", \"Cobb\", \"Samantha\", \"Michael\", \"Sarah\", \"Daniel\", \"Emily\", \"Joshua\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch for ambiguity and rivalry\n",
    "ambiguity = False #Set to True if you want to run the experiment with ambiguity\n",
    "rivalrous = False #Set to True if you want to run the experiment with rivalry   \n",
    "#Dynamic sub-instruction generation based on the parameters\n",
    "if ambiguity:\n",
    "    location_of_gems = f\"\"\"Each mountain could hold a Diamond, a Ruby, or a Topaz. However, which mountains contain which gems is unknown, and nobody knows how many gems per type there are. At the beginning of each round, each type of gem is randomly assigned to a different mountain, so any gem could be hidden in any mountain.\"\"\"\n",
    "else:\n",
    "    location_of_gems = f\"\"\"In each round, there are: {MOUNTAINS_DISTRIBUTION.count(\"low\")} mountain(s) containing topazes; {MOUNTAINS_DISTRIBUTION.count(\"medium\")} mountain(s) containing rubies; {MOUNTAINS_DISTRIBUTION.count(\"high\")} mountain(s) containing diamonds. However, which mountains contain which gems is unknown. At the beginning of each round, each type of gem is randomly assigned to a different mountain, so any gem could be hidden in any mountain.\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent instructions are defined below, with the respective placeholders according to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8068478450664a229f28bbfa5f280dcb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 88,
    "execution_start": 1715143368725,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "non_rivalrous_instructions = f\"\"\"\n",
    "General Information:\n",
    "Welcome. This is an experiment in the economics of decision-making. If you pay close attention to these instructions, you can earn a significant amount of money paid to you at the end of the experiment. Following these instructions, you will be asked to make some choices. There are no correct choices. Your choices depend on your preferences and beliefs, so different participants will usually make different choices. You will be paid according to your choices, so read these instructions carefully and think before you decide.\n",
    "\n",
    "The Basic Idea:\n",
    "There are {len(MOUNTAINS_DISTRIBUTION)} mountains and each of them hides one type of gem, which can only be found by exploring the mountain. There are 3 types of gems hidden in the {len(MOUNTAINS_DISTRIBUTION)} mountains: Diamonds, Rubies, and Topazes. The exact values of the topazes, rubies, and diamonds vary across rounds but the diamonds are always worth more than the rubies and the rubies are always worth more than the topazes. You choose which mountains to explore and the value of the gems you find are your earnings in dollars. Your objective is to maximize your own earnings.\n",
    "\n",
    "Location of Gems:\n",
    "{location_of_gems}\n",
    "\n",
    "How Participants Choose Mountains:\n",
    "There are {no_of_players} participants including you in total. In each round, participants choose which mountain to explore. The choice does not happen simultaneously, but participants choose sequentially, one after the other, according to a random order. You can choose to explore any mountain you wish. If you choose the same mountain chosen by other participants, each of you will receive the full value uncovered. Similarly, if someone else chooses the same mountain that you previously chose, you will still receive the full gem's value (and so will the other participant(s) who chose it). This means that payoffs are non-rival and there is no penalty in choosing the same mountain as other players. To repeat, no participant has any private information in Stage 1 on the location of the gems.\n",
    "\n",
    "Each Round Has 2 Stages:\n",
    "A round consists of 2 stages. At the beginning of a new round, gems are randomly allocated to the {len(MOUNTAINS_DISTRIBUTION)} different mountains. The position of gems will not be reset between the two stages in a round. In Stage 1, all participants sequentially choose one mountain to explore. Before choosing a mountain, you will see which mountains have been selected by the other participants in your group who chose before you. You can choose the same mountain chosen by other participants or a different mountain. At the end of Stage 1, the gems hidden in each mountain selected by all participants in Stage 1 are revealed, and you earn the value of the gem hidden in the mountain you chose. In Stage 2, you can again choose any of the same {len(MOUNTAINS_DISTRIBUTION)} mountains; that is, you can either choose the same mountain from Stage 1 or switch to another one. The position of gems remains the same as in Stage 1, but this time you will also see the gems located in the mountains revealed in Stage 1. At the end of Stage 2, the gems hidden in each mountain selected by all participants in Stage 2 are revealed, and you earn the value of the gem hidden in the mountain you chose in Stage 2. Your total earnings for the round equal the sum of the value of the gem you found in Stage 1 and the value of the gem you found in Stage 2. Again, if multiple players choose the same mountain, they all receive its full value.\n",
    "\n",
    "Payment:\n",
    "At the end of the round, you will be paid an amount equivalent to the sum of payoffs you earned in Stage 1 and Stage 2. This protocol of determining payments suggests that you should choose in each Stage knowing that your choice directly determines your payment because the dollar value of the gems you select will directly translate into your earnings. \n",
    "\n",
    "Frequently Asked Questions:\n",
    "Q1: Is this some kind of psychology experiment with an agenda you haven't told us?A: No, it is an economics experiment. These instructions are meant to clarify how you earn money and our interest is in seeing how people make decisions.\n",
    "Q2: Is there a ''correct'' or ''wrong'' choice of action? Is this kind of a test?A: No, your optimal choice depends on your preferences and beliefs and different people may hold different beliefs.\n",
    "Q3: Will there be any mountains with empty payoffs (no gem at all)? A: No, each and every mountain contains a gem and you are guaranteed a payoff for choosing any mountain whether it is a topaz, ruby, or a diamond.\n",
    "Q4: Will there be any negative payoffs for some hidden mountains?A: No, there is no potentially lower payoff than of a topaz, which is always positive.\n",
    "Q5: Are the payoffs split if more players choose the same mountain?A: No, each participant receives the full value of the gem uncovered, regardless of how many participants choose a mountain in that specific stage.\n",
    "\n",
    "Instructions for the Risk Preference:\n",
    "The Choice: You will be asked to choose between two options, \"Option A\" and \"Option B\" where:\n",
    "\n",
    "\"Option A\" always pays $4.00 with probability p and $3.20 otherwise.\n",
    "\"Option B\" always pays $7.70 with probability p and $0.20 otherwise.\n",
    "\n",
    "Repeated Choices:\n",
    "\n",
    "You will be asked to make a choice between \"Option A\" and \"Option B\" not once, but ten times where p will increase from 10% to 100%, 10% at a time.\n",
    "For example, the first choice will have p=10% and you will choose whether you prefer \"Option A\" ($4.00 with a 10% chance or $3.20 otherwise) or \"Option B\" ($7.70 with a 10% chance or $0.20 otherwise).\n",
    "\n",
    "Each successive choice will increase p by 10 percentage points until the last choice where \"Option A\" will pay $4.00 with certainty, and \"Option B\" will pay $7.70 with certainty.\n",
    "Note: Once you switch from choosing \"Option A\" to \"Option B\", it makes sense that you will continue to choose \"Option B\" in all consecutive choice problems. For example, if you prefer \"Option B\" when p=80%, then it makes sense to prefer \"Option B\" when p=90% and when p=100%, since \"Option B\" is even more attractive in these choice problems.\n",
    "\n",
    "Payment for risk preference task:\n",
    "The computer will randomly select one of the 10 choice problems and pay you according to your choice in that problem where the computer will decide the outcome based on the value of p. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivalrous_instructions = f\"\"\"\n",
    "General Information:\n",
    "Welcome. This is an experiment in the economics of decision-making. If you pay close attention to these instructions, you can earn a significant amount of money paid to you at the end of the experiment. Following these instructions, you will be asked to make some choices. There are no correct choices. Your choices depend on your preferences and beliefs, so different participants will usually make different choices. You will be paid according to your choices, so read these instructions carefully and think before you decide.\n",
    "\n",
    "The Basic Idea:\n",
    "There are {len(MOUNTAINS_DISTRIBUTION)} mountains and each of them hides one type of gem, which can only be found by exploring the mountain. There are 3 types of gems hidden in the {len(MOUNTAINS_DISTRIBUTION)} mountains: Diamonds, Rubies, and Topazes. The exact values of the topazes, rubies, and diamonds vary across rounds but the diamonds are always worth more than the rubies and the rubies are always worth more than the topazes. You choose which mountains to explore and the value of the gems you find influence your earnings in dollars. Your objective is to maximize your own earnings.\n",
    "\n",
    "Location of Gems:\n",
    "{location_of_gems}\n",
    "\n",
    "How Participants Choose Mountains:\n",
    "There are {no_of_players} participants including you in total. In each round, participants choose which mountain to explore. The choice does not happen simultaneously, but participants choose sequentially, one after the other, according to a random order. You can choose to explore any mountain you wish. If you choose the same mountain chosen by other participants, each of you will equally split the value of the uncovered gem in that specific stage. Similarly, if someone else chooses the same mountain that you previously chose, you will still receive the split value of the gem (and so will the other participant(s) who chose it) in that specific stage. This means that payoffs are rival and there is some penalty in choosing the same mountain as other players. To repeat, no participant has any private information in Stage 1 on the location of the gems.\n",
    "\n",
    "Each Round Has 2 Stages:\n",
    "A round consists of 2 stages. At the beginning of a new round, gems are randomly allocated to the {len(MOUNTAINS_DISTRIBUTION)} different mountains. The position of gems will not be reset between the two stages in a round. In Stage 1, all participants sequentially choose one mountain to explore. Before choosing a mountain, you will see which mountains have been selected by the other participants in your group who chose before you. You can choose the same mountain chosen by other participants or a different mountain. At the end of Stage 1, the gems hidden in each mountain selected by all participants in Stage 1 are revealed, and you earn the payoffs based on the gem hidden in the mountain you chose and whether other players chose the same mountain or not in that specific stage. In Stage 2, you can again choose any of the same {len(MOUNTAINS_DISTRIBUTION)} mountains; that is, you can either choose the same mountain from Stage 1 or switch to another one. The position of gems remains the same as in Stage 1, but this time you will also see the gems located in the mountains revealed in Stage 1. At the end of Stage 2, the gems hidden in each mountain selected by all participants in Stage 2 are revealed, and you earn the split value of the gem hidden in the mountain you chose in Stage 2. Your total earnings for the round equal the sum of the split value of the gem you found in Stage 1 and the split value of the gem you found in Stage 2. Again, if multiple players choose the same mountain, they all equally split its full value in that specific stage.\n",
    "\n",
    "Payment:\n",
    "At the end of the round, you will be paid an amount equivalent to the sum of payoffs you earned in Stage 1 and Stage 2. This protocol of determining payments suggests that you should choose in each Stage knowing that your choice directly determines your payment because the dollar value of the gems you select will directly translate into your earnings. \n",
    "\n",
    "Frequently Asked Questions:\n",
    "Q1: Is this some kind of psychology experiment with an agenda you haven't told us?A: No, it is an economics experiment. These instructions are meant to clarify how you earn money and our interest is in seeing how people make decisions.\n",
    "Q2: Is there a ''correct'' or ''wrong'' choice of action? Is this kind of a test?A: No, your optimal choice depends on your preferences and beliefs and different people may hold different beliefs.\n",
    "Q3: Will there be any mountains with empty payoffs (no gem at all)? A: No, each and every mountain contains a gem and you are guaranteed a payoff for choosing any mountain whether it is a topaz, ruby, or a diamond.\n",
    "Q4: Will there be any negative payoffs for some hidden mountains?A: No, there is no potentially lower payoff than of a topaz, which is always positive.\n",
    "Q5: Are the payoffs split if more players choose the same mountain?A: Yes, the payoffs are split depending on how many participants choose a mountain in that specific stage. For instance, if 3 agents choose a mountain of value $12 in stage 1, they all get $4 each for that stage. Similarly, if 2 agents choose a mountain with a payoff of $12 in stage 2, they get $6 each for stage 2.\n",
    "\n",
    "Instructions for the Risk Preference:\n",
    "The Choice: You will be asked to choose between two options, \"Option A\" and \"Option B\" where:\n",
    "\n",
    "\"Option A\" always pays $4.00 with probability p and $3.20 otherwise.\n",
    "\"Option B\" always pays $7.70 with probability p and $0.20 otherwise.\n",
    "\n",
    "Repeated Choices:\n",
    "\n",
    "You will be asked to make a choice between \"Option A\" and \"Option B\" not once, but ten times where p will increase from 10% to 100%, 10% at a time.\n",
    "For example, the first choice will have p=10% and you will choose whether you prefer \"Option A\" ($4.00 with a 10% chance or $3.20 otherwise) or \"Option B\" ($7.70 with a 10% chance or $0.20 otherwise).\n",
    "\n",
    "Each successive choice will increase p by 10 percentage points until the last choice where \"Option A\" will pay $4.00 with certainty, and \"Option B\" will pay $7.70 with certainty.\n",
    "Note: Once you switch from choosing \"Option A\" to \"Option B\", it makes sense that you will continue to choose \"Option B\" in all consecutive choice problems. For example, if you prefer \"Option B\" when p=80%, then it makes sense to prefer \"Option B\" when p=90% and when p=100%, since \"Option B\" is even more attractive in these choice problems.\n",
    "\n",
    "Payment for risk preference task:\n",
    "The computer will randomly select one of the 10 choice problems and pay you according to your choice in that problem where the computer will decide the outcome based on the value of p. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rivalrous:\n",
    "    instructions = rivalrous_instructions\n",
    "else:\n",
    "    instructions = non_rivalrous_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create agents with the respective traits and instructions\n",
    "agents = [Agent(\n",
    "        name=name, \n",
    "        traits={\"age\": age, \"location\": location, \"agent_id\": agent_id},\n",
    "        instruction=instructions) for name, age, location, agent_id in zip(names, ages, locations, agent_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-experiment quiz to check the understanding of the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = QuestionMultipleChoice(\n",
    "   question_name = \"same_mountain\",\n",
    "   question_text = \"In each round, you will select two mountains (one in Stage 1, and one in Stage 2) and collect the gem that they hide. You can choose the same mountain in both stages, or change after Stage 1?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")\n",
    "q2 = QuestionMultipleChoice(\n",
    "   question_name = \"value_split\",\n",
    "   question_text = \"If more than one player selects the same mountain, the value of the gem will be split among all the participants who chose it and there is a competition among the players?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")\n",
    "q3 = QuestionMultipleChoice(\n",
    "   question_name = \"gems_shuffling\",\n",
    "   question_text = \"At the beginning of a new round, the gems are reshuffled and randomly allocated to a different mountain?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")\n",
    "q4 = QuestionMultipleChoice(\n",
    "   question_name = \"private_information\",\n",
    "   question_text = \"No group member has any private initial information in Stage 1 on the location of gems?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")\n",
    "q5 = QuestionMultipleChoice(\n",
    "   question_name = \"stage_2_reset\",\n",
    "   question_text = \"The position of gems will not be reset between the two stages of a round?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")\n",
    "q6 = QuestionMultipleChoice(\n",
    "   question_name = \"simultaneous_or_not\",\n",
    "   question_text = \"All group members choose their mountain simultaneously?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")\n",
    "q7 = QuestionMultipleChoice(\n",
    "   question_name = \"group_member_before_you\",\n",
    "   question_text = \"If another group member chose a mountain before you, you cannot choose it again?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")\n",
    "q8 = QuestionMultipleChoice(\n",
    "   question_name = \"round_payment_random\",\n",
    "   question_text = \"Each and every mountain has a gem and no mountain is empty at all. Also a topaz gem has the lowest payoff and there is nothing worse than that?\",\n",
    "   question_options = [\"Correct\", \"Incorrect\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quiz = Survey(questions = [q1, q2, q3, q4, q5, q6, q7, q8])\n",
    "quiz = quiz.set_full_memory_mode()\n",
    "c = Cache() # create empty Cache object\n",
    "\n",
    "quiz_results = quiz.by(agents).by(model).run(cache = c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct answers for the quiz based on the parameters\n",
    "if rivalrous:\n",
    "    correct_answers = ['Correct', 'Correct', 'Correct', 'Correct', 'Correct', 'Incorrect', 'Incorrect', 'Correct']\n",
    "else:\n",
    "    correct_answers = ['Correct', 'Incorrect', 'Correct', 'Correct', 'Correct', 'Incorrect', 'Incorrect', 'Correct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_names = []\n",
    "question_one_answer = []\n",
    "question_one_comment = []\n",
    "question_two_answer = []\n",
    "question_two_comment = []\n",
    "question_three_answer = []\n",
    "question_three_comment = []\n",
    "question_four_answer = []\n",
    "question_four_comment = []\n",
    "question_five_answer = []\n",
    "question_five_comment = []\n",
    "question_six_answer = []\n",
    "question_six_comment = []\n",
    "question_seven_answer = []\n",
    "question_seven_comment = []\n",
    "question_eight_answer = []\n",
    "question_eight_comment = []\n",
    "\n",
    "# Gather quiz results\n",
    "for item in quiz_results:\n",
    "    agent_names.append(item['agent']['name'])\n",
    "    question_one_answer.append(item['answer']['same_mountain'])\n",
    "    question_one_comment.append(item['answer']['same_mountain_comment'])\n",
    "    question_two_answer.append(item['answer']['value_split'])\n",
    "    question_two_comment.append(item['answer']['value_split_comment'])\n",
    "    question_three_answer.append(item['answer']['gems_shuffling'])\n",
    "    question_three_comment.append(item['answer']['gems_shuffling_comment'])\n",
    "    question_four_answer.append(item['answer']['private_information'])\n",
    "    question_four_comment.append(item['answer']['private_information_comment'])\n",
    "    question_five_answer.append(item['answer']['stage_2_reset'])\n",
    "    question_five_comment.append(item['answer']['stage_2_reset_comment'])\n",
    "    question_six_answer.append(item['answer']['simultaneous_or_not'])\n",
    "    question_six_comment.append(item['answer']['simultaneous_or_not_comment'])\n",
    "    question_seven_answer.append(item['answer']['group_member_before_you'])\n",
    "    question_seven_comment.append(item['answer']['group_member_before_you_comment'])\n",
    "    question_eight_answer.append(item['answer']['round_payment_random'])\n",
    "    question_eight_comment.append(item['answer']['round_payment_random_comment'])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'agent_names': agent_names,\n",
    "    'question_1_answer': question_one_answer,\n",
    "    'question_1_comment': question_one_comment,\n",
    "    'question_2_answer': question_two_answer,\n",
    "    'question_2_comment': question_two_comment,\n",
    "    'question_3_answer': question_three_answer,\n",
    "    'question_3_comment': question_three_comment,\n",
    "    'question_4_answer': question_four_answer,\n",
    "    'question_4_comment': question_four_comment,\n",
    "    'question_5_answer': question_five_answer,\n",
    "    'question_5_comment': question_five_comment,\n",
    "    'question_6_answer': question_six_answer,\n",
    "    'question_6_comment': question_six_comment,\n",
    "    'question_7_answer': question_seven_answer,\n",
    "    'question_7_comment': question_seven_comment,\n",
    "    'question_8_answer': question_eight_answer,\n",
    "    'question_8_comment': question_eight_comment\n",
    "})\n",
    "\n",
    "# Function to calculate scores\n",
    "def calculate_score(row):\n",
    "    return sum([row[f'question_{i}_answer'] == correct_answers[i-1] for i in range(1, 9)])\n",
    "\n",
    "# Apply function to calculate scores\n",
    "df['score'] = df.apply(calculate_score, axis=1)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('agent_scores_no-data.csv', index=False)\n",
    "\n",
    "# Update agent traits with quiz results\n",
    "for i, item in enumerate(quiz_results):\n",
    "    text = \"Quiz results:\\n\"\n",
    "    questions_answers_comments = [\n",
    "        (1, question_one_answer[i], question_one_comment[i], correct_answers[0]),\n",
    "        (2, question_two_answer[i], question_two_comment[i], correct_answers[1]),\n",
    "        (3, question_three_answer[i], question_three_comment[i], correct_answers[2]),\n",
    "        (4, question_four_answer[i], question_four_comment[i], correct_answers[3]),\n",
    "        (5, question_five_answer[i], question_five_comment[i], correct_answers[4]),\n",
    "        (6, question_six_answer[i], question_six_comment[i], correct_answers[5]),\n",
    "        (7, question_seven_answer[i], question_seven_comment[i], correct_answers[6]),\n",
    "        (8, question_eight_answer[i], question_eight_comment[i], correct_answers[7]),\n",
    "    ]\n",
    "    for q_num, answer, comment, correct in questions_answers_comments:\n",
    "        text += f\"Question Number {q_num}:\\n\"\n",
    "        text += f\"Question: {df.columns[df.columns.get_loc(f'question_{q_num}_answer') - 1]}\\n\"\n",
    "        text += f\"Your Answer: {answer}\\n\"\n",
    "        text += f\"Correct Answer: {correct}\\n\"\n",
    "        text += f\"Your rationale: {comment}\\n\"\n",
    "    agents[i]['traits']['quiz'] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(text, text_count, total_length):\n",
    "    if text_count > total_length:\n",
    "        raise ValueError(\"The text count cannot be greater than the total length of the array.\")\n",
    "    \n",
    "    # Create an array with the specified number of text entries\n",
    "    array = [text] * text_count\n",
    "    \n",
    "    # Add 'None' to fill the rest of the array\n",
    "    array.extend(['None'] * (total_length - text_count))\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk_priming = ['This agent enjoys taking high risks with the potential for high rewards. This means the agent will go for high payoff choices even though they might end up with low outcomes.', 'This agent enjoys taking high risks with the potential for high rewards. This means the agent will go for high payoff choices even though they might end up with low outcomes.', 'This agent enjoys taking high risks with the potential for high rewards. This means the agent will go for high payoff choices even though they might end up with low outcomes.', 'This agent balances risk and reward, considering both potential gains and potential losses. The agent is likely to choose moderate-risk options that offer a reasonable chance of a good outcome without extreme potential for loss. They weigh the probabilities and impacts of different choices carefully, aiming for a balance between safety and opportunity. ', 'This agent balances risk and reward, considering both potential gains and potential losses. The agent is likely to choose moderate-risk options that offer a reasonable chance of a good outcome without extreme potential for loss. They weigh the probabilities and impacts of different choices carefully, aiming for a balance between safety and opportunity. ', 'This agent balances risk and reward, considering both potential gains and potential losses. The agent is likely to choose moderate-risk options that offer a reasonable chance of a good outcome without extreme potential for loss. They weigh the probabilities and impacts of different choices carefully, aiming for a balance between safety and opportunity.', 'This agent prefers to avoid uncertainty and potential losses. This means the agent will choose safer, low-risk options even if the potential rewards are smaller. ', 'This agent prefers to avoid uncertainty and potential losses. This means the agent will choose safer, low-risk options even if the potential rewards are smaller. ', 'This agent prefers to avoid uncertainty and potential losses. This means the agent will choose safer, low-risk options even if the potential rewards are smaller. ','This agent prefers to avoid uncertainty and potential losses. This means the agent will choose safer, low-risk options even if the potential rewards are smaller. ']\n",
    "risk_priming = [\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\"]\n",
    "# risk_text = \"\"\"This agent is risk-loving. If the diamond has not been found yet, they will pick an unchosen mountain to give them the chance of discovering one, since this is the kind of risk they like to take.\"\"\"\n",
    "# text_count = 1\n",
    "# total_length = 10\n",
    "\n",
    "# risk_priming = create_array(risk_text, text_count, total_length)\n",
    "\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "    agent['traits']['risk_priming'] = risk_priming[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosociality = [\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\"]\n",
    "\n",
    "\n",
    "# prosocial_text = \"\"\"This agent is pro-social and aims to help the other players. This means they prefer unchosen mountains to give the other members of the group the chance of discovering a diamond.\"\"\"\n",
    "# text_count = 5\n",
    "# total_length = 10\n",
    "# prosociality = create_array(prosocial_text, text_count, total_length)\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "    agent['traits']['prosociality'] = prosociality[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_priming = [\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\",\"None\"]\n",
    "\n",
    "# exploration_text = \"\"\"This agent only wants to find the diamond. They will keep exploring until they find the diamond, since this is their ultimate goal.\"\"\"\n",
    "# text_count = 5\n",
    "# total_length = 10\n",
    "# exploration_priming = create_array(exploration_text, text_count, total_length)\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "    agent['traits']['exploration_priming'] = exploration_priming[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Risk preference survey\n",
    "#10 questions, 10% increments from 10% to 100%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_1\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 10%, $3.20 otherwise, Option B - $7.70 with a chance of 10%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q2_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_2\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 20%, $3.20 otherwise, Option B - $7.70 with a chance of 20%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q3_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_3\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 30%, $3.20 otherwise, Option B - $7.70 with a chance of 30%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q4_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_4\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 40%, $3.20 otherwise, Option B - $7.70 with a chance of 40%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q5_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_5\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 50%, $3.20 otherwise, Option B - $7.70 with a chance of 50%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q6_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_6\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 60%, $3.20 otherwise, Option B - $7.70 with a chance of 60%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q7_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_7\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 70%, $3.20 otherwise, Option B - $7.70 with a chance of 70%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q8_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_8\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 80%, $3.20 otherwise, Option B - $7.70 with a chance of 80%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q9_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_9\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 90%, $3.20 otherwise, Option B - $7.70 with a chance of 90%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n",
    "\n",
    "q10_risk = QuestionMultipleChoice(\n",
    "    question_name = \"risk_question_10\",\n",
    "    question_text = \"Which option do you choose: Option A - $4.00 with a chance of 100%, $3.20 otherwise, Option B - $7.70 with a chance of 100%, $0.20 otherwise\",\n",
    "    question_options = [\"A\", \"B\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_preference = Survey(questions = [q1_risk, q2_risk, q3_risk, q4_risk, q5_risk, q6_risk, q7_risk, q8_risk, q9_risk, q10_risk])\n",
    "risk_preference = risk_preference.set_full_memory_mode()\n",
    "c = Cache() # create empty Cache object\n",
    "\n",
    "risk_preference_results = risk_preference.by(agents).by(Model('gpt-4-turbo')).run(cache = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    q1_risk, q2_risk, q3_risk, q4_risk, q5_risk, q6_risk, q7_risk, q8_risk, q9_risk, q10_risk\n",
    "]\n",
    "\n",
    "# Initialize lists\n",
    "agent_names = []\n",
    "risk_question_answers = [[] for _ in range(10)]\n",
    "risk_question_comments = [[] for _ in range(10)]\n",
    "\n",
    "# Iterate over the risk preference results\n",
    "for item in risk_preference_results:\n",
    "    agent_names.append(item['agent']['name'])\n",
    "    for i in range(len(questions)):\n",
    "        risk_question_answers[i].append(item['answer'][f'risk_question_{i+1}'])\n",
    "        risk_question_comments[i].append(item['answer'][f'risk_question_{i+1}_comment'])\n",
    "    \n",
    "    # Create a text format combining all the questions, agent's answers, and correct answers\n",
    "    text = \"Risk Preferences results:\\n\"\n",
    "    for i in range(len(questions)):\n",
    "        question_text = questions[i]['question_text']\n",
    "        answer = risk_question_answers[i][-1]\n",
    "        comment = risk_question_comments[i][-1]\n",
    "        text += f\"Question Number {i+1}:\\nQuestion: {question_text}\\nYour Answer: {answer}\\nYour Rationale: {comment}\\n\"\n",
    "    \n",
    "    # Add the text to the agent's traits\n",
    "    item['agent']['traits']['risk_survey'] = text\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'agent_names': agent_names\n",
    "}\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    data[f'risk_question_{i+1}_answer'] = risk_question_answers[i]\n",
    "    data[f'risk_question_{i+1}_comment'] = risk_question_comments[i]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('risk_preferences_no-data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the traits of the agents\n",
    "agents[1]['traits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_response(agent_id, context, agents):\n",
    "    \n",
    "    c = Cache() # create empty Cache object\n",
    "    question_options = [\"Mountain 1\", \"Mountain 2\", \"Mountain 3\", \"Mountain 4\", \"Mountain 5\"]\n",
    "    random.shuffle(question_options)\n",
    "    q = QuestionMultipleChoice(\n",
    "        question_name = \"select_mountain\",\n",
    "        question_text = f\"You are being asked to choose a mountain. Context: {context}\\n Which mountain do you choose?\",\n",
    "        question_options = question_options\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = q.by(agents[agent_id]).by(model).run(cache = c)\n",
    "        result.show_exceptions(traceback=True)\n",
    "        agent_choice = result.select(\"answer.select_mountain\").first()\n",
    "        agent_comment = result.select(\"comment.select_mountain_comment\").first()\n",
    "    except:\n",
    "        agent_choice = None\n",
    "        agent_comment = \"Picked a choice completely random, without any rational due to an error.\"\n",
    "\n",
    "    # To show the prompts that were used:\n",
    "    # result.select(\"agent_name\", \"prompt.select_mountain_system_prompt\").print()\n",
    "    # result.select(\"agent_name\", \"prompt.select_mountain_system_prompt\", \"prompt.select_mountain_user_prompt\", \"answer.select_mountain\", \"comment.select_mountain_comment\").print(format=\"rich\")\n",
    "\n",
    "    # Return just the response:\n",
    "    return agent_choice, agent_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing master file or create a new one if it doesn't exist\n",
    "try:\n",
    "    master_data = pd.read_csv(\"master_data_points_no-data.csv\")\n",
    "    print(\"Loaded existing master data file.\")\n",
    "except FileNotFoundError:\n",
    "    master_data = pd.DataFrame(columns=[\"round_number\",\"condition\",\"risk_priming\",\"ambiguity\",\"rivalrous\",\"revealed_mountain\",\"sub_round_number\",\"stage_number\",\"agent_id\", \"choice\", \"choice_order\", \"comment_by_the_agent\", \"agent_payoff\", \"total_payoff_of_the_mountain\", \"payoff_class\", \"mountains_arrangement\",\"mountains_payoffs\", \"prosociality\", \"exploration_priming\", \"error_flag\"])\n",
    "    print(\"Created new master data file.\")\n",
    "\n",
    "# Your existing data_points DataFrame\n",
    "data_points = pd.DataFrame(columns=[\"round_number\",\"condition\",\"risk_priming\",\"ambiguity\",\"rivalrous\",\"revealed_mountain\",\"sub_round_number\",\"stage_number\",\"agent_id\", \"choice\", \"choice_order\", \"comment_by_the_agent\", \"agent_payoff\", \"total_payoff_of_the_mountain\", \"payoff_class\", \"mountains_arrangement\",\"mountains_payoffs\", \"prosociality\", \"exploration_priming\", \"error_flag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "210b340bfd5d4937a6b4019ee9d78922",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22922,
    "execution_start": 1715143368825,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Function to simulate the game\n",
    "def simulate_game(overall_round_number,sub_round_number,mountain_values,reveal_type, agent_list):\n",
    "    mountains = random.sample(MOUNTAINS_DISTRIBUTION, len(MOUNTAINS_DISTRIBUTION))\n",
    "    revealed_value = None\n",
    "    agents_context_p1={}\n",
    "    agents_context_p2={}\n",
    "    mountain_map = {}  # Dictionary to store mountain index to value mappings\n",
    "    inverse_mountains_payoffs = {v: k for k, v in mountain_values.items()} # Invert the dictionary so values become keys\n",
    "\n",
    "    # Reveal data if applicable\n",
    "    if reveal_type in mountain_values:\n",
    "        for mountain in mountains:\n",
    "            if mountain_values[mountain] == mountain_values[reveal_type]:\n",
    "                # revealed_value = (mountains.index(mountain), mountain_values[mountain])\n",
    "                revealed_value = (mountains.index(mountain), GEMS_MAPPING[reveal_type])\n",
    "                mountain_map[\"Mountain \"+str(mountains.index(mountain)+1)] = mountain_values[mountain]  # Store revealed mountain mapping\n",
    "                break\n",
    "\n",
    "    print(f\"Mountains (hidden for the agents): {mountains}\")\n",
    "\n",
    "    if ambiguity:\n",
    "        context = \"Mountains are hidden in a random order. Payoffs are hidden in mountains in a random order and all the mountains have a gem of a certain value, with  Diamond > Ruby > Topaz.\"\n",
    "    else:\n",
    "        context = f\"\"\" This round has three payoffs of value ${mountain_values['low']} (Topazes), \n",
    "       one value of ${mountain_values['medium']} (Ruby) and one value of ${mountain_values['high']} (Diamond).\n",
    "        Mountains are hidden in a random order. Payoffs are hidden in mountains in a random order and all the mountains have a gem of a certain value, with  Diamond > Ruby > Topaz. \n",
    "        \"\"\"\n",
    "    if revealed_value:\n",
    "        # print(f\"Revealed mountain: Mountain {revealed_value[0]+1} with value ${revealed_value[1]}\")\n",
    "        # context = context + f\"Revealed Mountain: Mountain {revealed_value[0]+1} with value ${revealed_value[1]} \\n\"\n",
    "        print(f\"Revealed mountain: Mountain {revealed_value[0]+1} has a gem {revealed_value[1]}\")\n",
    "        context = context + f\"Revealed Mountain: Mountain {revealed_value[0]+1} has a gem {revealed_value[1]} \\n\"\n",
    "    else:\n",
    "        print(\"No mountains are revealed for this round\")\n",
    "        context = context + \"No mountains are revealed for this round \\n\"\n",
    "    \n",
    "    agent_order = [x - 1 for x in agent_list]  # List of agent indices\n",
    "    random.shuffle(agent_order)   # Shuffle the list\n",
    "\n",
    "    # Agents make choices for stage 1\n",
    "    choices = []\n",
    "    print(\"Agents are making their choices for Stage 1...\\n\")\n",
    "    context = context + \"Agents are making their choices for Stage 1... \\n\"\n",
    "    for j,i in enumerate(agent_order): \n",
    "        agents_context_p1[i+1] = context\n",
    "        choice, comment_by_the_agent = agent_response(i,context, agents)\n",
    "        if choice:\n",
    "            choice = str(choice)\n",
    "            error_flag = 0\n",
    "        else:\n",
    "            choice = random.choice([\"Mountain 1\", \"Mountain 2\", \"Mountain 3\", \"Mountain 4\", \"Mountain 5\"])\n",
    "            choice = str(choice)\n",
    "            error_flag = 1\n",
    "        choices.append(choice)\n",
    "        mountain_index = int(re.findall('\\d+\\.\\d+|\\d+', choice)[0]) - 1\n",
    "        mountain_map[choice] = mountain_values[mountains[mountain_index]]  # Update mountain mapping with actual value\n",
    "        temp = f\"Agent {i+1}, enter your choice of mountain: {choice}\"\n",
    "        context =  context + f\"An Agent chose {choice}\" + \"\\n\" \n",
    "        print(temp)\n",
    "        payoff_value = mountain_values[mountains[int(re.findall('\\d+\\.\\d+|\\d+', choice)[0]) - 1]]\n",
    "        data_points.loc[len(data_points)] = [overall_round_number,reveal_type, agents[i]['traits']['risk_priming'],ambiguity, rivalrous, \"None\", sub_round_number, 1, i+1, choice, j+1, comment_by_the_agent, 0, payoff_value, inverse_mountains_payoffs.get(payoff_value), mountains, mountain_values, agents[i]['traits']['prosociality'], agents[i]['traits']['exploration_priming'], error_flag]\n",
    "        master_data.loc[len(data_points)] = [overall_round_number,reveal_type, agents[i]['traits']['risk_priming'],ambiguity, rivalrous, \"None\", sub_round_number, 1, i+1, choice, j+1, comment_by_the_agent, 0, payoff_value, inverse_mountains_payoffs.get(payoff_value), mountains, mountain_values, agents[i]['traits']['prosociality'], agents[i]['traits']['exploration_priming'], error_flag]\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"\\nStage 1 Choices (by position) of peers: {[x for x in choices]}\")\n",
    "    \n",
    "    if rivalrous:\n",
    "        # Share payoffs if multiple agents choose the same mountain\n",
    "        choice_counts = {choice: choices.count(choice) for choice in set(choices)}\n",
    "        outcomes = [mountain_values[mountains[int(re.findall('\\d+\\.\\d+|\\d+', choice)[0]) - 1]] / choice_counts[choice] for choice in choices]\n",
    "    else:\n",
    "        outcomes = [mountain_values[mountains[int(re.findall('\\d+\\.\\d+|\\d+', choice)[0]) - 1]] for choice in choices]\n",
    "\n",
    "    # Update outcomes in data_points for Stage 1\n",
    "    for idx, (choice, outcome) in enumerate(zip(choices, outcomes)):\n",
    "        data_points.loc[(data_points['round_number'] == overall_round_number) &\n",
    "                        (data_points['sub_round_number'] == sub_round_number) &\n",
    "                        (data_points['stage_number'] == 1) &\n",
    "                        (data_points['agent_id'] == agent_order[idx] + 1), 'agent_payoff'] = outcome\n",
    "    print(\"Learnings from Stage 1:\")\n",
    "    context = context + \"Learnings from Stage 1:\\n\"\n",
    "\n",
    "    for index, value in mountain_map.items():\n",
    "        print(f\"{index} is revealed to have a value of ${value}\")\n",
    "        context = context + f\"{index} is revealed to have a value of ${value} \\n\"\n",
    "\n",
    "\n",
    "    # Agents make choices for stage 2\n",
    "    print(\"\\nAgents are making their choices for Stage 2...\")\n",
    "    context = context + \"Agents are making their choices for Stage 2... \\n\"\n",
    "\n",
    "    new_choices = []\n",
    "    for j,i in enumerate(agent_order):\n",
    "        agents_context_p2[i+1] = context\n",
    "        choice, comment_by_the_agent = agent_response(i,context, agents)\n",
    "        if choice:\n",
    "            choice = str(choice)\n",
    "            error_flag = 0\n",
    "        else:\n",
    "            choice = random.choice([\"Mountain 1\", \"Mountain 2\", \"Mountain 3\", \"Mountain 4\", \"Mountain 5\"])\n",
    "            choice = str(choice)\n",
    "            error_flag = 1\n",
    "        new_choices.append(choice)\n",
    "        temp = f\"Agent {i+1}, enter your choice of mountain: {choice}\"\n",
    "        context = context + f\"An Agent chose {choice}\\n\"\n",
    "        print(temp)\n",
    "        payoff_value = mountain_values[mountains[int(re.findall('\\d+\\.\\d+|\\d+', choice)[0]) - 1]]\n",
    "        data_points.loc[len(data_points)] = [overall_round_number, reveal_type, agents[i]['traits']['risk_priming'],ambiguity, rivalrous, \"None\", sub_round_number, 2, i+1, choice, j+1, comment_by_the_agent, 0, payoff_value, inverse_mountains_payoffs.get(payoff_value), mountains,mountain_values, agents[i]['traits']['prosociality'], agents[i]['traits']['exploration_priming'], error_flag]\n",
    "        master_data.loc[len(data_points)] = [overall_round_number, reveal_type, agents[i]['traits']['risk_priming'],ambiguity, rivalrous, \"None\", sub_round_number, 2, i+1, choice, j+1, comment_by_the_agent, 0, payoff_value, inverse_mountains_payoffs.get(payoff_value), mountains,mountain_values, agents[i]['traits']['prosociality'], agents[i]['traits']['exploration_priming'], error_flag]\n",
    "\n",
    "    \n",
    "    print(f\"\\nStage 2 Choices: {[x for x in new_choices]}\")\n",
    "\n",
    "    if rivalrous:\n",
    "        # Share payoffs if multiple agents choose the same mountain\n",
    "        new_choice_counts = {choice: new_choices.count(choice) for choice in set(new_choices)}\n",
    "        new_outcomes = [mountain_values[mountains[int(re.findall('\\d+\\.\\d+|\\d+', choice)[0]) - 1]] / new_choice_counts[choice] for choice in new_choices]\n",
    "    else:\n",
    "        new_outcomes = [mountain_values[mountains[int(re.findall('\\d+\\.\\d+|\\d+', choice)[0]) - 1]] for choice in new_choices]\n",
    "\n",
    "    # Update outcomes in data_points for Stage 2\n",
    "    for idx, (choice, outcome) in enumerate(zip(new_choices, new_outcomes)):\n",
    "        data_points.loc[(data_points['round_number'] == overall_round_number) &\n",
    "                        (data_points['sub_round_number'] == sub_round_number) &\n",
    "                        (data_points['stage_number'] == 2) &\n",
    "                        (data_points['agent_id'] == agent_order[idx] + 1), 'agent_payoff'] = outcome \n",
    "\n",
    "    print(f\"Stage 2 Outcomes: {new_outcomes}\")\n",
    "    \n",
    "    # Calculate total payoffs\n",
    "    total_payoffs = [x + y for x, y in zip(outcomes, new_outcomes)]\n",
    "    payoffs_dict = {agent: payoff for agent, payoff in zip(agent_order, total_payoffs)}\n",
    "\n",
    "    for i in agent_order:\n",
    "        print(f\"\\nTotal payoff for Agent {i+1}: ${payoffs_dict[i]}\")\n",
    "        print(f\"Their Stage 1 context: {agents_context_p1[i+1]}\\nTheir Stage 2 context: {agents_context_p2[i+1]}\")\n",
    "    \n",
    "    # After each game, append new data to master file\n",
    "    master_data.to_csv(\"master_data_points_no-data.csv\", index=False)\n",
    "    print(f\"Updated master data file. Total rows: {len(master_data)}\")\n",
    "\n",
    "    # Your existing code to save individual round data\n",
    "    data_points.to_csv(f\"v1_no-data_points-data_Seq.csv\", index=False)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_values_list = [{'low': 1, 'medium': 6, 'high': 10}, {'low': 1, 'medium': 6, 'high': 10.5}, {'low': 1, 'medium': 7, 'high': 12}, {'low': 2, 'medium': 7, 'high': 11}, {'low': 3, 'medium': 8, 'high': 12}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mountain_values_list = [{'low': 1, 'medium': 5, 'high': 10}, {'low': 1, 'medium': 5, 'high': 10.5}, {'low': 1, 'medium': 6, 'high': 12}, {'low': 2, 'medium': 6, 'high': 11}, {'low': 3, 'medium': 7, 'high': 12}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 20\n",
    "overall_round_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to shuffle and select agents, if you want to run the experiment with a different number of agents, you can modify the following function\n",
    "# def get_shuffled_agents(agent_ids, num_agents, num_groups):\n",
    "#     agents = []\n",
    "#     for _ in range(num_groups):\n",
    "#         random.shuffle(agent_ids)\n",
    "#         for i in range(0, len(agent_ids), num_agents):\n",
    "#             agents.append(agent_ids[i:i + num_agents])\n",
    "#     return agents\n",
    "\n",
    "# # Generate the list of agents for each of the 5 groups\n",
    "# selected_agents_list = get_shuffled_agents(agent_ids, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through mountain values and game types\n",
    "for mountain_values in mountain_values_list:\n",
    "    for game_type in ('no-data',):  \n",
    "        for group in range(1):  # For each group of agents\n",
    "            # agent_list = selected_agents_list[group]  # Select the current group of agents\n",
    "            agent_list = [1,2,3,4,5]\n",
    "            for i in range(rounds):  # Each group plays 5 games\n",
    "                print(f\"Round {overall_round_number} and sub-round {i+1} of category {game_type} is going on\")\n",
    "                simulate_game(overall_round_number, i+1, mountain_values, reveal_type=game_type, agent_list=agent_list)\n",
    "                overall_round_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mountain_values in mountain_values_list:\n",
    "#     for game_type in ('no_data'):\n",
    "#         for i in range(rounds):\n",
    "#             print(f\"Round {overall_round_number} and sub-round {i+1} of category {game_type} is going on\")\n",
    "#             simulate_game(overall_round_number, i+1,mountain_values,reveal_type=game_type, agent_list=agent_list)\n",
    "#             overall_round_number = overall_round_number + 1"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "90aac1da59224e0db831764c7ebdca0a",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
